{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 10\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "## Deadlines\n",
    "\n",
    "- Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "- Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "### Notes\n",
    "- Reviews will be assigned on Monday, December 4\n",
    "- You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Literal, Union\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "DirectIndex = Literal[0,1,2,3,4,5,6,7,8]\n",
    "RowColIndex = tuple[Literal[0,1,2], Literal[0,1,2]]\n",
    "Move = Union[DirectIndex, RowColIndex]\n",
    "Cell = Literal[-1, 0, 1]\n",
    "PlayerIndex = Literal[0,1]\n",
    "BoardHash = str\n",
    "\n",
    "CELL_TO_EMOJI=(\"⬜\",\"❎\",\"⏺️\")\n",
    "CELL_TO_CHAR=(\"B\", \"X\", \"O\")\n",
    "\n",
    "@dataclass(repr=False)\n",
    "class Board:\n",
    "    board: np.ndarray = field(default_factory=lambda: np.ones(9, dtype=np.int8) * -1)\n",
    "\n",
    "    @staticmethod\n",
    "    def i_to_rc(i: DirectIndex) -> RowColIndex:\n",
    "        return i//3, i % 3\n",
    "    \n",
    "    @staticmethod\n",
    "    def rc_to_i(rc: RowColIndex) -> DirectIndex:\n",
    "        r, c = rc\n",
    "        return r*3 + c\n",
    "\n",
    "    @staticmethod\n",
    "    def is_valid_index(idx: Move) -> bool:\n",
    "        if isinstance(idx, tuple):\n",
    "            return idx[0] >= 0 and idx[0] <= 2 and idx[1]>=0 and idx[1]<= 2\n",
    "        else:\n",
    "            return idx >= 0 and idx <= 8\n",
    "        \n",
    "    def __getitem__(self, idx: Move) -> Cell:\n",
    "        \"\"\"Access the cell directly with index or row-col\"\"\"\n",
    "        assert Board.is_valid_index(idx), \"Invalid Index: {idx}\"\n",
    "        if isinstance(idx, tuple):\n",
    "            idx = Board.rc_to_i(idx)\n",
    "        return self.board[idx]\n",
    "\n",
    "    def __setitem__(self, idx: Move, value: Cell) -> None:\n",
    "        assert Board.is_valid_index(idx), \"Invalid Index: {idx}\"\n",
    "        if isinstance(idx, tuple):\n",
    "            idx = Board.rc_to_i(idx)\n",
    "        self.board[idx] = value\n",
    "\n",
    "    def is_valid_move(self: \"Board\",move: Move) -> bool:\n",
    "        return self[move] == -1\n",
    "    \n",
    "    def move(self: \"Board\", player: \"PlayerIndex\", move: Move) -> bool:\n",
    "        valid = self[move] == -1 \n",
    "        if valid:\n",
    "            self[move] = player\n",
    "        return valid\n",
    "    \n",
    "    def is_playable(self: \"Board\") -> bool:\n",
    "        return any(self.board == -1) and self.won() == -1\n",
    "    \n",
    "    def won(self: \"Board\") -> Literal[0, 1, -1]:\n",
    "        \"\"\"Check if someone has won\"\"\"\n",
    "\n",
    "        rows = [[0,1,2], [3,4,5], [6,7,8]]\n",
    "        cols = [[0,3,6],[1,4,7], [2,5,8]]\n",
    "        diag = [[0,4,8], [2,4,6]]\n",
    "        all_ = [*rows, *cols, *diag]\n",
    "\n",
    "        if any(all(self.board[c] == 0) for c in all_):\n",
    "            return 0\n",
    "        elif any(all(self.board[c] == 1) for c in all_):\n",
    "            return 1\n",
    "        else: \n",
    "            return -1\n",
    "    \n",
    "    def __repr__(self: \"Board\") -> str:\n",
    "        winner = self.won()\n",
    "        return f\"Board({str(self.board)}, {winner=}) \"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Pretty print the board\"\"\"\n",
    "        s = \"\"\n",
    "        for r in range(3):\n",
    "            for c in range(3):\n",
    "                s += CELL_TO_EMOJI[self[(r,c)] + 1]\n",
    "            s+=\"\\n\"\n",
    "        winner = self.won()\n",
    "        if winner != -1:\n",
    "            s += f\"Winner: Player {winner}\"\n",
    "        return s\n",
    "    \n",
    "    def hash(self: \"Board\", plind: PlayerIndex) -> BoardHash: \n",
    "        \"\"\"Stringified version of the board, so it can be used as a dict key\"\"\"\n",
    "        return \"\".join([CELL_TO_CHAR[c+1] for c in self.board]) + str(plind)\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_hash(s: BoardHash) -> \"Board\":\n",
    "        assert len(s) >= 9, \"Invalid board\"\n",
    "        b: list[int]\n",
    "        try:\n",
    "            b = [CELL_TO_CHAR.index(c)-1 for c in s[:9]]\n",
    "        except ValueError:\n",
    "            raise AssertionError(\"InvalidError\")\n",
    "        return Board(np.array(b))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp(value, min_, max_):\n",
    "    \"\"\"Clamp value between min_ and max_\"\"\"\n",
    "    return min(max(value, min_), max_)\n",
    "\n",
    "def avg(iterable):\n",
    "    return sum(iterable)/len(iterable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player(ABC):\n",
    "    \"\"\"Abstract Player class\"\"\"\n",
    "    \n",
    "    @property \n",
    "    @abstractmethod\n",
    "    def name(self: \"Player\") -> str:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def choose_move(self, board: \"Board\", player_index: PlayerIndex) -> Move:\n",
    "        raise NotImplementedError\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game(player0: \"Player\", player1: \"Player\", verbose: bool = False) -> Literal[-1, 0, 1]:\n",
    "    \"\"\"Play a single game\"\"\"\n",
    "    board = Board()\n",
    "    if verbose: \n",
    "        print(board)\n",
    "    players = [player0, player1]\n",
    "    plind: PlayerIndex = 1\n",
    "    while board.is_playable():\n",
    "        plind = 1-plind\n",
    "        player = players[plind]\n",
    "        move = None\n",
    "        while move is None or not board.is_valid_move(move):\n",
    "            move = player.choose_move(board, plind)\n",
    "        board.move(plind, move)\n",
    "        if verbose:\n",
    "            print(board)\n",
    "    return board.won()\n",
    "\n",
    "def benchmark(player_to_benchmark: \"Player\", opponent: \"Player\", games: int = 100, *, quiet: bool = False) -> tuple[float, float, float]:\n",
    "    \"\"\"Benchmark a player, in both position\"\"\"\n",
    "    wins_as_first, wins_as_second = 0, 0\n",
    "    draws_as_first, draws_as_second = 0, 0\n",
    "    for i in range(games):\n",
    "        if i % 2 == 0:\n",
    "            end = game(player_to_benchmark, opponent)\n",
    "            wins_as_first += 1 if end == 0 else 0\n",
    "            draws_as_first += 1 if end == -1 else 0\n",
    "        else:\n",
    "            end = game(opponent, player_to_benchmark)\n",
    "            wins_as_second += 1 if end == 1 else 0\n",
    "            draws_as_second += 1 if end == -1 else 0\n",
    "    acc, first_acc, sec_acc = (wins_as_first + wins_as_second) / games, wins_as_first*2/games, wins_as_second*2/games\n",
    "    draw_acc, draw_first_acc, draw_sec_acc = (wins_as_first + wins_as_second + draws_as_first + draws_as_second) / games, (wins_as_first+draws_as_first)*2/games, (wins_as_second+draws_as_second)*2/games\n",
    "    if not quiet:\n",
    "        print(f\"[{player_to_benchmark.name} vs {opponent.name} for {games} games]\")\n",
    "        print(f\"        Wins: {acc:.2%}, {first_acc:.2%} as first, {sec_acc:.2%} as second\")\n",
    "        print(f\"Wins + Draws: {draw_acc:.2%}, {draw_first_acc:.2%} as first, {draw_sec_acc:.2%} as second\")\n",
    "    else:\n",
    "        return (acc, first_acc, sec_acc), (draw_acc, draw_first_acc, draw_sec_acc)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Player and Human Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AndyDwyer(Player):\n",
    "    \"\"\"Random Player\"\"\"\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"Andy Dwyer\"\n",
    "\n",
    "    def choose_move(self, board, player_index) -> DirectIndex:\n",
    "        \"\"\"Make random move\"\"\"\n",
    "        return random.randrange(0,9)\n",
    "    \n",
    "@dataclass\n",
    "class TomHaverford(Player):\n",
    "    \"\"\"Human Player, I wanted to have fun :)\"\"\"\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"Tom Haverford\"\n",
    "\n",
    "    def choose_move(self, board, player_index) -> DirectIndex:\n",
    "        print(board)\n",
    "        while True:\n",
    "            inp = input(f\"{CELL_TO_EMOJI[player_index+1]} choose your move (row, column):\")\n",
    "            try:\n",
    "                r, c = inp.split(\",\")\n",
    "                r = int(r.strip())\n",
    "                c = int(c.strip())\n",
    "                return Board.rc_to_i((r,c))\n",
    "            except:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RonSwanson(Player):\n",
    "    \"\"\"Q-Learning Player\"\"\"\n",
    "\n",
    "    learning_rate: float = field(default=0.1)\n",
    "    discount_rate: float = field(default=0.99)\n",
    "    exploration_rate: float = field(default=1)\n",
    "    min_exploration_rate: float= field(default=0.01)\n",
    "    exploration_decay_rate: float= field(default=2.5e-5)\n",
    "    num_of_episodes: int = field(default=1_000)\n",
    "    qtable: dict[BoardHash, list[float]] = field(default_factory=lambda: defaultdict(lambda: [0]*9), repr=False)\n",
    "\n",
    "    @property\n",
    "    def name(self): \n",
    "        return \"Ron Swanson\"\n",
    "\n",
    "    def reward(self, type: Literal[\"action\", \"game\"], board: \"Board\", *, move: Move = None, player_position: PlayerIndex = None) -> float:\n",
    "        assert type in [\"action\", \"game\"], \"Invalid reward type\"\n",
    "        if type == \"action\":\n",
    "            assert move is not None, \"Cannot retrieve reward for action if no move is provided\"\n",
    "            return 1 if board.is_valid_move(move) else float('-inf')\n",
    "        else:\n",
    "            assert player_position is not None, \"Cannot retrieve reward for game if no player position is provided\"\n",
    "            won = board.won() \n",
    "            draw = won == -1\n",
    "            if draw: return 0\n",
    "            else:\n",
    "                return 10 if won == player_position else -10\n",
    "        \n",
    "    def training_move_chooser(self, board: \"Board\", player_position: PlayerIndex) -> Move:\n",
    "        if random.uniform(0, 1) > self.exploration_rate:\n",
    "            # exploit\n",
    "            if board.hash(plind=player_position) in self.qtable:\n",
    "                return np.argmax(self.qtable[board.hash(player_position)])\n",
    "        # explore or nothing to exploit\n",
    "        return random.randrange(0, 9)    \n",
    "\n",
    "    def train(self: \"RonSwanson\", opponent: \"Player\" = None, verbose: bool = False):\n",
    "        if opponent is None:\n",
    "            opponent = AndyDwyer()\n",
    "        rewards_per_episode = [0] * self.num_of_episodes\n",
    "        pbar = trange(self.num_of_episodes, unit=\"episode\", desc=f\"Training against {opponent.name}\")\n",
    "\n",
    "        if not verbose:\n",
    "            vprint = lambda x: None\n",
    "        else:\n",
    "            vprint = print\n",
    "        for episode in pbar:\n",
    "            board = Board()\n",
    "            if episode % 2 == 0:\n",
    "                whoami = 0\n",
    "            else:\n",
    "                whoami = 1\n",
    "            plind: PlayerIndex = 1\n",
    "\n",
    "            previous_board_hash: BoardHash \n",
    "            next_board_hash: BoardHash \n",
    "            move: Move\n",
    "            \n",
    "            while board.is_playable():\n",
    "                plind = 1-plind\n",
    "                if whoami == plind:\n",
    "                    move_was_valid = False\n",
    "                    vprint(f\"{self.name}'s turn ({plind})\")\n",
    "                    while not move_was_valid:\n",
    "                        move = self.training_move_chooser(board, plind)\n",
    "                        reward = self.reward(\"action\", board, move=move)\n",
    "                        rewards_per_episode[episode] += reward\n",
    "                        previous_board_hash = board.hash(plind)\n",
    "                        move_was_valid = board.move(plind, move)\n",
    "                        next_board_hash = board.hash(plind)\n",
    "                        vprint(f\"{self.name} is picking: {move=},{reward=},{previous_board_hash=},{next_board_hash=}\")\n",
    "\n",
    "                        self.qtable[previous_board_hash][move] *= 1-self.learning_rate\n",
    "                        self.qtable[previous_board_hash][move] += self.learning_rate * (\n",
    "                            reward + self.discount_rate * np.max(self.qtable[next_board_hash])\n",
    "                            )\n",
    "                else:\n",
    "                    opponent_move: Move = None\n",
    "                    vprint(f\"{opponent.name}'s turn ({plind})\")\n",
    "                    while opponent_move is None or not board.is_valid_move(opponent_move):\n",
    "                        opponent_move = opponent.choose_move(board, plind)\n",
    "                    board.move(plind, opponent_move)\n",
    "                    next_board_hash = board.hash(plind)\n",
    "\n",
    "            \n",
    "            reward = self.reward(\"game\", board, player_position=whoami)\n",
    "            rewards_per_episode[episode] += reward\n",
    "            self.qtable[previous_board_hash][move] *= 1-self.learning_rate\n",
    "            self.qtable[previous_board_hash][move] += self.learning_rate * (\n",
    "                reward + self.discount_rate * np.max(self.qtable[next_board_hash])\n",
    "                )\n",
    "\n",
    "            self.exploration_rate = clamp(np.exp(-self.exploration_decay_rate * episode), self.min_exploration_rate, 1)\n",
    "            if episode % 1000 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    \"Explored\": len(self.qtable.keys())\n",
    "                    })\n",
    "\n",
    "        return rewards_per_episode\n",
    "    \n",
    "    def choose_move(self, board: Board, player_index: PlayerIndex) -> Move:\n",
    "        if board.hash(player_index) in self.qtable:\n",
    "            move = np.argmax(self.qtable[board.hash(player_index)])\n",
    "            if board.is_valid_move(move):\n",
    "                return move\n",
    "        return random.randrange(0,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training against Andy Dwyer: 100%|██████████| 100000/100000 [04:21<00:00, 382.66episode/s, Explored=9994]\n"
     ]
    }
   ],
   "source": [
    "qlearning = RonSwanson(num_of_episodes=100_000)\n",
    "rewards_history = qlearning.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ron Swanson vs Andy Dwyer for 1000 games]\n",
      "        Wins: 84.90%, 95.20% as first, 74.60% as second\n",
      "Wins + Draws: 97.00%, 99.80% as first, 94.20% as second\n"
     ]
    }
   ],
   "source": [
    "benchmark(qlearning, AndyDwyer(), games=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computational-OmqOwF93-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
