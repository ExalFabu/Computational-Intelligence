{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 10\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "## Deadlines\n",
    "\n",
    "- Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "- Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "### Notes\n",
    "- Reviews will be assigned on Monday, December 4\n",
    "- You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Literal, Union\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import pickle\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DirectIndex = Literal[0,1,2,3,4,5,6,7,8]\n",
    "RowColIndex = tuple[Literal[0,1,2], Literal[0,1,2]]\n",
    "Move = Union[DirectIndex, RowColIndex]\n",
    "Cell = Literal[-1, 0, 1]\n",
    "PlayerIndex = Literal[0,1]\n",
    "BoardHash = str\n",
    "\n",
    "CELL_TO_EMOJI=(\"⬜\",\"❎\",\"⏺️\")\n",
    "CELL_TO_CHAR=(\"B\", \"X\", \"O\")\n",
    "def charify(arr) -> str:\n",
    "    \"\"\"Convert board array to hash-compatible string representation\"\"\"\n",
    "    return \"\".join([CELL_TO_CHAR[i + 1] for i in arr])\n",
    "\n",
    "\n",
    "CANONICAL_REPRESENTATION: bool = False\n",
    "\n",
    "@dataclass(repr=False)\n",
    "class Board:\n",
    "    board: np.ndarray = field(default_factory=lambda: np.ones(9, dtype=np.int8) * -1)\n",
    "    use_canonical: bool = field(default_factory= lambda: CANONICAL_REPRESENTATION)\n",
    "\n",
    "    @staticmethod\n",
    "    def i_to_rc(i: DirectIndex) -> RowColIndex:\n",
    "        return i//3, i % 3\n",
    "    \n",
    "    @staticmethod\n",
    "    def rc_to_i(rc: RowColIndex) -> DirectIndex:\n",
    "        r, c = rc\n",
    "        return r*3 + c\n",
    "\n",
    "    @staticmethod\n",
    "    def is_valid_index(idx: Move) -> bool:\n",
    "        if isinstance(idx, tuple):\n",
    "            return idx[0] >= 0 and idx[0] <= 2 and idx[1]>=0 and idx[1]<= 2\n",
    "        else:\n",
    "            return idx >= 0 and idx <= 8\n",
    "        \n",
    "    def __getitem__(self, idx: Move) -> Cell:\n",
    "        \"\"\"Access the cell directly with index or row-col\"\"\"\n",
    "        assert Board.is_valid_index(idx), \"Invalid Index: {idx}\"\n",
    "        if isinstance(idx, tuple):\n",
    "            idx = Board.rc_to_i(idx)\n",
    "        return self.board[idx]\n",
    "\n",
    "    def __setitem__(self, idx: Move, value: Cell) -> None:\n",
    "        assert Board.is_valid_index(idx), \"Invalid Index: {idx}\"\n",
    "        if isinstance(idx, tuple):\n",
    "            idx = Board.rc_to_i(idx)\n",
    "        self.board[idx] = value\n",
    "\n",
    "    def is_valid_move(self: \"Board\",move: Move) -> bool:\n",
    "        return self[move] == -1\n",
    "    \n",
    "    def move(self: \"Board\", player: \"PlayerIndex\", move: Move) -> bool:\n",
    "        valid = self[move] == -1 \n",
    "        if valid:\n",
    "            self[move] = player\n",
    "        return valid\n",
    "    \n",
    "    def is_playable(self: \"Board\") -> bool:\n",
    "        return any(self.board == -1) and self.won() == -1\n",
    "    \n",
    "    def won(self: \"Board\") -> Literal[0, 1, -1]:\n",
    "        \"\"\"Check if someone has won\"\"\"\n",
    "\n",
    "        rows = [[0,1,2], [3,4,5], [6,7,8]]\n",
    "        cols = [[0,3,6],[1,4,7], [2,5,8]]\n",
    "        diag = [[0,4,8], [2,4,6]]\n",
    "        all_ = [*rows, *cols, *diag]\n",
    "\n",
    "        if any(all(self.board[c] == 0) for c in all_):\n",
    "            return 0\n",
    "        elif any(all(self.board[c] == 1) for c in all_):\n",
    "            return 1\n",
    "        else: \n",
    "            return -1\n",
    "    \n",
    "    def canonical(self) -> tuple[\"Board\", Literal[1,2,3,4]]:\n",
    "        as_mat = self.board.reshape((3,3))\n",
    "        rots = [(charify(np.rot90(as_mat, k=i+1).flatten()), i+1) for i in range(4)]\n",
    "        canonical, idx = sorted(rots, key=lambda x: x[0])[0]\n",
    "        canonical = [CELL_TO_CHAR.index(c)-1 for c in canonical[:9]]\n",
    "        return Board(np.array(canonical)), idx\n",
    "    \n",
    "    def __repr__(self: \"Board\") -> str:\n",
    "        winner = self.won()\n",
    "        return f\"Board({str(self.board)}, {winner=}) \"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Pretty print the board\"\"\"\n",
    "        s = \"\"\n",
    "        for r in range(3):\n",
    "            for c in range(3):\n",
    "                s += CELL_TO_EMOJI[self[(r,c)] + 1]\n",
    "            s+=\"\\n\"\n",
    "        winner = self.won()\n",
    "        if winner != -1:\n",
    "            s += f\"Winner: Player {winner}\"\n",
    "        return s\n",
    "    \n",
    "    def hash(self: \"Board\", plind: PlayerIndex) -> BoardHash: \n",
    "        \"\"\"Stringified version of the board, so it can be used as a dict key\"\"\"\n",
    "\n",
    "        if not self.use_canonical:\n",
    "            return charify(self.board) + str(plind)\n",
    "        else:\n",
    "            return charify(self.canonical()[0].board) + str(plind)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_hash(s: BoardHash) -> \"Board\":\n",
    "        assert len(s) >= 9, \"Invalid board\"\n",
    "        b: list[int]\n",
    "        try:\n",
    "            b = [CELL_TO_CHAR.index(c)-1 for c in s[:9]]\n",
    "        except ValueError:\n",
    "            raise AssertionError(\"InvalidError\")\n",
    "        return Board(np.array(b))\n",
    "    \n",
    "    @staticmethod \n",
    "    def from_canonical(canonical: \"Board\", idx: Literal[1,2,3,4]) -> \"Board\":\n",
    "        b = np.rot90(canonical.board.reshape((3,3)), k = 4-idx).flatten()\n",
    "        return Board(np.array(b))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp(value, min_, max_):\n",
    "    \"\"\"Clamp value between min_ and max_\"\"\"\n",
    "    return min(max(value, min_), max_)\n",
    "\n",
    "def avg(iterable):\n",
    "    return sum(iterable)/len(iterable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player(ABC):\n",
    "    \"\"\"Abstract Player class\"\"\"\n",
    "    \n",
    "    @property \n",
    "    @abstractmethod\n",
    "    def name(self: \"Player\") -> str:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def choose_move(self, board: \"Board\", player_index: PlayerIndex) -> Move:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game(player0: \"Player\", player1: \"Player\", verbose: bool = False) -> Literal[-1, 0, 1]:\n",
    "    \"\"\"Play a single game\"\"\"\n",
    "    board = Board()\n",
    "    if verbose: \n",
    "        print(board)\n",
    "    players = [player0, player1]\n",
    "    plind: PlayerIndex = 1\n",
    "    while board.is_playable():\n",
    "        plind = 1-plind\n",
    "        player = players[plind]\n",
    "        move = None\n",
    "        while move is None or not board.is_valid_move(move):\n",
    "            move = player.choose_move(board, plind)\n",
    "        board.move(plind, move)\n",
    "        if verbose:\n",
    "            print(board)\n",
    "    return board.won()\n",
    "\n",
    "def benchmark(player_to_benchmark: \"Player\", opponent: \"Player\", games: int = 100, *, quiet: bool = False) -> tuple[float, float, float]:\n",
    "    \"\"\"Benchmark a player, in both position\"\"\"\n",
    "    wins_as_first, wins_as_second = 0, 0\n",
    "    draws_as_first, draws_as_second = 0, 0\n",
    "    for i in range(games):\n",
    "        if i % 2 == 0:\n",
    "            end = game(player_to_benchmark, opponent)\n",
    "            wins_as_first += 1 if end == 0 else 0\n",
    "            draws_as_first += 1 if end == -1 else 0\n",
    "        else:\n",
    "            end = game(opponent, player_to_benchmark)\n",
    "            wins_as_second += 1 if end == 1 else 0\n",
    "            draws_as_second += 1 if end == -1 else 0\n",
    "    acc, first_acc, sec_acc = (wins_as_first + wins_as_second) / games, wins_as_first*2/games, wins_as_second*2/games\n",
    "    draw_acc, draw_first_acc, draw_sec_acc = (wins_as_first + wins_as_second + draws_as_first + draws_as_second) / games, (wins_as_first+draws_as_first)*2/games, (wins_as_second+draws_as_second)*2/games\n",
    "    if not quiet:\n",
    "        print(f\"[{player_to_benchmark.name} vs {opponent.name} for {games} games]\")\n",
    "        print(f\"        Wins: {acc:.2%}, {first_acc:.2%} as first, {sec_acc:.2%} as second\")\n",
    "        print(f\"Wins + Draws: {draw_acc:.2%}, {draw_first_acc:.2%} as first, {draw_sec_acc:.2%} as second\")\n",
    "    else:\n",
    "        return (acc, first_acc, sec_acc), (draw_acc, draw_first_acc, draw_sec_acc)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Player and Human Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AndyDwyer(Player):\n",
    "    \"\"\"Random Player\"\"\"\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"Andy Dwyer\"\n",
    "\n",
    "    def choose_move(self, board, player_index) -> DirectIndex:\n",
    "        \"\"\"Make random move\"\"\"\n",
    "        return random.randrange(0,9)\n",
    "    \n",
    "@dataclass\n",
    "class TomHaverford(Player):\n",
    "    \"\"\"Human Player, I wanted to have fun :)\"\"\"\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"Tom Haverford\"\n",
    "\n",
    "    def choose_move(self, board, player_index) -> DirectIndex:\n",
    "        print(board)\n",
    "        while True:\n",
    "            inp = input(f\"{CELL_TO_EMOJI[player_index+1]} choose your move (row, column):\")\n",
    "            try:\n",
    "                r, c = inp.split(\",\")\n",
    "                r = int(r.strip())\n",
    "                c = int(c.strip())\n",
    "                return Board.rc_to_i((r,c))\n",
    "            except:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entry_default():\n",
    "    \"\"\"Needed for the object to be pickable\"\"\"\n",
    "    return [0] * 9\n",
    "\n",
    "def qtable_default():\n",
    "    \"\"\"Needed for the object to be pickable\"\"\"\n",
    "\n",
    "\n",
    "    return defaultdict(entry_default)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RonSwanson(Player):\n",
    "    \"\"\"Q-Learning Player\"\"\"\n",
    "\n",
    "    learning_rate: float = field(default=0.1)\n",
    "    discount_rate: float = field(default=0.99)\n",
    "    exploration_rate: float = field(default=1)\n",
    "    min_exploration_rate: float= field(default=0.01)\n",
    "    exploration_decay_rate: float= field(default=2.5e-5)\n",
    "    num_of_episodes: int = field(default=1_000)\n",
    "    qtable: dict[BoardHash, list[float]] = field(default_factory=qtable_default, repr=False)\n",
    "    # qtable: dict[BoardHash, list[float]] = field(default_factory=lambda: defaultdict(lambda: [0]*9), repr=False)\n",
    "\n",
    "    @property\n",
    "    def name(self): \n",
    "        return \"Ron Swanson\"\n",
    "\n",
    "    def reward(self, type: Literal[\"action\", \"game\"], board: \"Board\", *, move: Move = None, player_position: PlayerIndex = None) -> float:\n",
    "        assert type in [\"action\", \"game\"], \"Invalid reward type\"\n",
    "        if type == \"action\":\n",
    "            assert move is not None, \"Cannot retrieve reward for action if no move is provided\"\n",
    "            return 1 if board.is_valid_move(move) else float('-inf')\n",
    "        else:\n",
    "            assert player_position is not None, \"Cannot retrieve reward for game if no player position is provided\"\n",
    "            won = board.won() \n",
    "            draw = won == -1\n",
    "            if draw: return 0\n",
    "            else:\n",
    "                return 10 if won == player_position else -10\n",
    "        \n",
    "    def training_move_chooser(self, board: \"Board\", player_position: PlayerIndex) -> Move:\n",
    "        if random.uniform(0, 1) > self.exploration_rate:\n",
    "            # exploit\n",
    "            if board.hash(plind=player_position) in self.qtable:\n",
    "                return np.argmax(self.qtable[board.hash(player_position)])\n",
    "        # explore or nothing to exploit\n",
    "        return random.randrange(0, 9)    \n",
    "\n",
    "    def train(self: \"RonSwanson\", opponent: \"Player\" = None, verbose: bool = False, canonical: bool = None):\n",
    "        if opponent is None:\n",
    "            opponent = AndyDwyer()\n",
    "        if canonical is None:\n",
    "            canonical = CANONICAL_REPRESENTATION\n",
    "        rewards_per_episode = [0] * self.num_of_episodes\n",
    "        pbar = trange(self.num_of_episodes, unit=\"episode\", desc=f\"Training against {opponent.name}\")\n",
    "\n",
    "        if not verbose:\n",
    "            vprint = lambda x: None\n",
    "        else:\n",
    "            vprint = print\n",
    "        for episode in pbar:\n",
    "            board = Board(use_canonical=canonical)\n",
    "            if episode % 2 == 0:\n",
    "                whoami = 0\n",
    "            else:\n",
    "                whoami = 1\n",
    "            plind: PlayerIndex = 1\n",
    "\n",
    "            previous_board_hash: BoardHash \n",
    "            next_board_hash: BoardHash \n",
    "            move: Move\n",
    "            \n",
    "            while board.is_playable():\n",
    "                plind = 1-plind\n",
    "                if whoami == plind:\n",
    "                    move_was_valid = False\n",
    "                    vprint(f\"{self.name}'s turn ({plind})\")\n",
    "                    if canonical:\n",
    "                        # if canonical, play with the canonical board\n",
    "                        board, canon_idx = board.canonical()\n",
    "                    while not move_was_valid:\n",
    "                        move = self.training_move_chooser(board, plind)\n",
    "                        reward = self.reward(\"action\", board, move=move)\n",
    "                        rewards_per_episode[episode] += reward\n",
    "                        previous_board_hash = board.hash(whoami)\n",
    "                        move_was_valid = board.move(plind, move)\n",
    "                        next_board_hash = board.hash(whoami)\n",
    "                        vprint(f\"{self.name} is picking: {move=},{reward=},{previous_board_hash=},{next_board_hash=}\")\n",
    "\n",
    "                        # Update qtable\n",
    "                        self.qtable[previous_board_hash][move] *= 1-self.learning_rate\n",
    "                        self.qtable[previous_board_hash][move] += self.learning_rate * (reward + \n",
    "                                                                                        self.discount_rate * \n",
    "                                                                                        np.max(self.qtable[next_board_hash]))\n",
    "                    if canonical:\n",
    "                        # restore the non-canonical for the opponent\n",
    "                        board = Board.from_canonical(board, canon_idx)\n",
    "                else:\n",
    "                    opponent_move: Move = None\n",
    "                    vprint(f\"{opponent.name}'s turn ({plind})\")\n",
    "                    while opponent_move is None or not board.is_valid_move(opponent_move):\n",
    "                        opponent_move = opponent.choose_move(board, plind)\n",
    "                    board.move(plind, opponent_move)\n",
    "\n",
    "            reward = self.reward(\"game\", board, player_position=whoami)\n",
    "            rewards_per_episode[episode] += reward\n",
    "            self.qtable[previous_board_hash][move] *= 1-self.learning_rate\n",
    "            self.qtable[previous_board_hash][move] += self.learning_rate * (\n",
    "                reward + self.discount_rate * np.max(self.qtable[next_board_hash])\n",
    "                )\n",
    "\n",
    "            self.exploration_rate = clamp(np.exp(-self.exploration_decay_rate * episode), self.min_exploration_rate, 1)\n",
    "            if episode % int(self.num_of_episodes/100) == 0:\n",
    "                pbar.set_postfix({\n",
    "                    \"Explored\": len(self.qtable.keys())\n",
    "                    })\n",
    "\n",
    "        return rewards_per_episode\n",
    "    \n",
    "    def choose_move(self, board: Board, player_index: PlayerIndex) -> Move:\n",
    "        if board.hash(player_index) in self.qtable:\n",
    "            move = np.argmax(self.qtable[board.hash(player_index)])\n",
    "            if board.is_valid_move(move):\n",
    "                return move\n",
    "        return random.randrange(0,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning Player Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training against Andy Dwyer: 100%|██████████| 100000/100000 [03:09<00:00, 528.51episode/s, Explored=9983]\n"
     ]
    }
   ],
   "source": [
    "CANONICAL_REPRESENTATION = False\n",
    "filename = \"./basic_ron.pkl\"\n",
    "use_saved_obj: bool = True\n",
    "if use_saved_obj and path.isfile(path.abspath(filename)):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        qlearning = pickle.load(f)\n",
    "else:\n",
    "    qlearning = RonSwanson(num_of_episodes=100_000)\n",
    "    _ = qlearning.train()\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(qlearning, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ron Swanson vs Andy Dwyer for 1000 games]\n",
      "        Wins: 83.30%, 89.40% as first, 77.20% as second\n",
      "Wins + Draws: 96.60%, 98.80% as first, 94.40% as second\n"
     ]
    }
   ],
   "source": [
    "benchmark(qlearning, AndyDwyer(), games=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning with Canonical Representation Results\n",
    "In order to reduce the number of states, exploiting the symmetries in TicTacToe\n",
    "I've tried to use a canonical representation of the board. \\\n",
    "The canonical board is the one with the smallest lexicographical order among the \\\n",
    "boards obtained by applying all the possible rotations to the original board. \\\n",
    "The player then uses the canonical board to update the Q-table and to choose the next move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training against Andy Dwyer: 100%|██████████| 50000/50000 [03:15<00:00, 255.20episode/s, Explored=2520]\n"
     ]
    }
   ],
   "source": [
    "CANONICAL_REPRESENTATION = True\n",
    "filename = \"./canon_ron.pkl\"\n",
    "use_saved_obj: bool = True\n",
    "if use_saved_obj and path.isfile(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        with_canon = pickle.load(f)\n",
    "else:\n",
    "    with_canon = RonSwanson(num_of_episodes=50_000)\n",
    "    _ = with_canon.train(canonical=True)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(with_canon, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ron Swanson vs Andy Dwyer for 1000 games]\n",
      "        Wins: 52.00%, 66.20% as first, 37.80% as second\n",
      "Wins + Draws: 65.70%, 79.60% as first, 51.80% as second\n"
     ]
    }
   ],
   "source": [
    "benchmark(with_canon, AndyDwyer(), games=1_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Points\n",
    "1. Unfortunately, something went wrong while experimenting with the canonical representation, I am not sure what, but the player does not improve at all.\n",
    "2. Another thing that my colleagues and I noticed is that the player on the starting board seems to equally prefer all the possible moves, which is not what we logically expect. Are we wrong to suppose that the player should prefer some moves over others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('BBBBBBBBB0', [0.9999999999999994, 0.9999999999999994, 0.9999999999999994, 0.9999999999999994, 0.9999999999999994, 0.9999999999999994, 0.9999999999999994, 0.9999999999999994, 0.9999999999999994])\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "('BBBBBBBBB0', [0.9999999999999994, 0.9999999999999994, 0.9999999999999994, 0.9999999999999994, 0.9999999999999994, 0.9999999999999994, 0.9999999999999994, 0.9999999999999994, 0.9999999999999994])\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(list(qlearning.qtable.items())[0])\n",
    "print(np.array(list(qlearning.qtable.items())[0][1]).reshape((3,3)))\n",
    "print(list(with_canon.qtable.items())[0])\n",
    "print(np.array(list(with_canon.qtable.items())[0][1]).reshape((3,3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computational-OmqOwF93-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
